---
layout: post
title: Optimisation de Trajectoire UAV via RL pour le 3D Gaussian Splatting
date: 2025-12-09
---

## Objectif du projet
Le projet vise à optimiser la trajectoire de vol d’un drone par Apprentissage par Renforcement afin d’améliorer la qualité d’une reconstruction 3D basée sur le 3D Gaussian Splatting (3DGS). En exploitant un signal d’incertitude fourni par le modèle, le drone apprend à cibler les zones mal reconstruites. L’objectif est d’augmenter la couverture informative tout en minimisant le temps de vol.

**Objectif :** Apprendre.

<img src="{{ site.baseurl }}/images/prometheus.gif" alt="image" width="auto" height="auto" style="display: block; margin: 0 auto;">
> Je fixe une caméra Insta360 sur un drone et le tour est joué, probablement.

### Architecture du Pipeline

Pour atteindre cette autonomie, le système repose sur une boucle fermée entre la perception et l'action. Voici l'architecture globale envisagée (premier jet) :

<img src="{{ site.baseurl }}/images/UAV.png" alt="image" width="auto" height="auto" style="display: block; margin: 0 auto;">

1.  **Acquisition :** Le drone capture des images et des données inertielles.
2.  **Estimation & Reconstruction :** Le modèle 3DGS est initialisé sommairement, puis raffiné.
3.  **Analyse d'incertitude :** Question ouverte à explorer.
4.  **Décision (RL) :** L'agent reçoit cet état et calcule la prochaine meilleure position pour combler les lacunes.

## État des lieux et Premiers Benchmarks

Avant d'intégrer la boucle de contrôle RL, il est crucial de valider la chaîne de reconstruction 3DGS et d'identifier les goulots d'étranglement temporels. J'ai réalisé plusieurs tests comparatifs.

### 1. Test avec téléphone

J'ai d'abord testé une approche "grand public" pour établir une baseline de qualité et de vitesse.

- **Sujet :** **[panda roux](/scans/panda-roux.html)**, 
- **Outil :** Scaniverse et SuperSplat pour l'export.

> Sur la page du scan, double-cliquez sur le modèle pour une manipulation plus intuitive.

### 2. Le test haute fidélité : Fujifilm XT-2 + COLMAP

Pour se rapprocher des conditions d'un drone porteur d'une optique de qualité, j'ai utilisé un Fuji XT-2 (23mm f/2) et traité les images sur PC.

![Visualisation Zig](/images/zig.png)

**Le constat :**
Avec seulement 30 images traitées via le pipeline classique (**COLMAP** pour le Structure-from-Motion + **BRUSH**), les résultats sont mitigés et surtout très lents.

* **Scaniverse :** 2 min de vidéo ~2 à 5 min de traitement (Smartphone).
* **Pipeline PC :** 30 images ~20 min de traitement (RX 7800XT).

Cette lourdeur de calcul est un obstacle majeur pour du *Real-Time RL*.

---

## Pistes d'optimisation : Contourner le SfM

L'étape la plus coûteuse n'est pas l'entraînement des gaussiennes, mais le calcul de la pose des caméras (Structure from Motion - SfM). Comme le soulignait une discussion pertinente sur [**Reddit**](https://www.reddit.com/r/GaussianSplatting/comments/1mdjscn/how_is_the_scaniverse_app_even_possible/) :

> "La plupart du temps, créer un 3DGS revient à faire du Structure from Motion. Si vous connaissez l'ordre des images, faites du SLAM en temps réel et récupérez les données du gyroscope, vous pouvez éliminer la majeure partie du temps de SfM. [...] L'entraînement des splats est rapide. C'est tout ce qui précède qui prend du temps."

**Hypothèse de travail :**
Le drone disposant de capteurs embarqués précis (IMU, GPS, Odométrie), nous pouvons nous passer du calcul photogrammétrique pur de COLMAP.

### Solutions techniques envisagées

Pour réduire le temps de calcul côté reconstruction géométrique, je vais évaluer :

1.  **[GLOMAP](https://github.com/colmap/glomap)** : Une alternative moderne et plus efficace à COLMAP.
2.  **[CF-3DGS](https://github.com/NVlabs/CF-3DGS)** : Une approche utilisant la continuité temporelle des flux vidéo pour contourner COLMAP.

### Validation avec flux vidéo (Sequential Matching)

En appliquant cette logique sur une vidéo du Fuji XT-2 ([**voir le scan**](/scans/fuji.html)), j'ai utilisé ffmpeg pour extraire 1 image par seconde de la vidéo mais il serait intéressant de tester la différence de rendu en fonction du nombre d'images extraites.

L'utilisation du mode Sequential Matching de COLMAP a considérablement accéléré les calculs, confirmant que l'exploitation de la cohérence temporelle (vidéo vs photos en vrac) est la clé pour notre application.