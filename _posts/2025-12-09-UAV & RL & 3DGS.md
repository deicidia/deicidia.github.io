---
layout: post
title: UAV & RL pour 3DGS
date: 2025-12-09
---


Le projet vise à optimiser la trajectoire de vol d’un drone par Apprentissage par Renforcement (RL) afin d’améliorer la qualité d’une reconstruction 3D basée sur le 3D Gaussian Splatting (3DGS). En exploitant un signal d’incertitude fourni par le modèle, le drone apprend à cibler les zones mal reconstruites. L’objectif est d’augmenter la couverture informative tout en minimisant le temps de vol.

**Objectif :** Scanner une pièce avec un drone autonome.

<img src="{{ site.baseurl }}/images/prometheus.gif" alt="image" width="auto" height="auto" style="display: block; margin: 0 auto;">
> Je fixe une caméra Insta360 sur un drone et le tour est joué, probablement.

## Architecture du Pipeline

Pour atteindre cette autonomie, le système repose sur une boucle fermée entre la perception et l'action. Voici l'architecture globale envisagée (premier jet) :

<img src="{{ site.baseurl }}/images/UAV.png" alt="image" width="auto" height="auto" style="display: block; margin: 0 auto;">

1.  **Acquisition :** Le drone capture des images et des données inertielles.
2.  **Estimation & Reconstruction :** Le modèle 3DGS est initialisé sommairement, puis raffiné.
3.  **Analyse d'incertitude :** Question ouverte à explorer.
4.  **Décision (RL) :** L'agent reçoit cet état et calcule la prochaine meilleure position pour combler les lacunes.

## État des lieux et Premiers Benchmarks

Avant d'intégrer la boucle de contrôle RL, il est crucial de valider la chaîne de reconstruction 3DGS et d'identifier les goulots d'étranglement temporels. J'ai réalisé plusieurs tests comparatifs.

### Test avec téléphone

J'ai d'abord testé une approche "grand public" pour établir une baseline de qualité et de vitesse.

- **Sujet :** [**Panda roux**](/scans/panda-roux.html), 
- **Outil :** [**Scaniverse**](https://scaniverse.com/) et [**SuperSplat**](https://developer.playcanvas.com/user-manual/) pour l'export.

Sur la page du scan, double-cliquez sur le modèle pour une manipulation plus intuitive.

### Test haute fidélité : Fujifilm XT-2 + COLMAP

Pour se rapprocher des conditions d'un drone porteur d'une optique de qualité, j'ai utilisé un Fuji XT-2 (23mm f/2) et traité les images sur PC.

![Visualisation Zig](/images/zig.png)

#### Première expérience
Avec seulement 30 images traitées via le pipeline classique ([**COLMAP**](https://colmap.github.io/index.html) pour le Structure-from-Motion + [**BRUSH**](https://github.com/ArthurBrussee/brush)), les résultats sont mitigés et surtout très lents.

* **Scaniverse :** 2 min de vidéo ~5 à 10 min de traitement (Smartphone).
* **Pipeline PC :** 30 images ~20 min de traitement (RX 7800XT).

Cette lourdeur de calcul est un obstacle majeur pour du *Real-Time RL* mais est due au mode Exhaustive Matching de COLMAP qui compare toutes les images entre elles. 

#### Deuxième expérience
J'ai testé dans un second temps avec une vidéo du Fuji XT-2 ([**voir le scan**](/scans/fuji.html)). J'ai utilisé ffmpeg pour extraire 1 image par seconde de la vidéo mais il serait intéressant de tester la différence de rendu en fonction du nombre d'images extraites.

L'utilisation du mode Sequential Matching de COLMAP a considérablement accéléré les calculs, confirmant que l'exploitation de la cohérence temporelle (vidéo vs photos en vrac) est la clé pour notre application. J'ai aussi fait un scan de mon chat, c'est cadeau [**voir le scan**](/scans/zigoto.html).

Un problème notable est le fait que l'optique du XT2 n'est pas stabilisée, ce qui entraine des images de moins bonne qualité (flou de bougé). De plus le 23mm (équivalent ~35mm plein format) est en limite basse des recommandations habituelles pour la photogrammétrie d'objets ([**PlayCanvas**](https://developer.playcanvas.com/user-manual/gaussian-splatting/creating/taking-photos/#:~:text=35%2D85mm%20equivalent%20focal%20length) conseille 35-85mm eq.), bien qu'acceptable pour des scènes. Une focale plus longue et une ouverture entre f/8 et f/11 assureraient une meilleure netteté uniforme. 

---

## Pistes d'optimisation : Contourner le SfM

L'étape la plus coûteuse n'est pas l'entraînement des gaussiennes, mais le calcul de la pose des caméras (Structure from Motion - SfM). Comme le soulignait une discussion pertinente sur [**Reddit**](https://www.reddit.com/r/GaussianSplatting/comments/1mdjscn/how_is_the_scaniverse_app_even_possible/) :

> "La plupart du temps, créer un 3DGS revient à faire du Structure from Motion. Si vous connaissez l'ordre des images, faites du SLAM en temps réel et récupérez les données du gyroscope, vous pouvez éliminer la majeure partie du temps de SfM. [...] L'entraînement des splats est rapide. C'est tout ce qui précède qui prend du temps." 

Le drone disposant de capteurs embarqués précis (IMU, GPS, Odométrie), nous pouvons utiliser ces données pour initialiser les poses ou utiliser le mode [**Spatial Matching**](https://colmap.github.io/tutorial.html#:~:text=de/colmap/.-,Spatial%20Matching,-%3A%20This%20matching%20mode) de COLMAP pour accélérer drastiquement l'étape de *Matching*, voire contourner le SfM complet en utilisant un SLAM visuel embarqué.

### Solutions techniques envisagées

Pour réduire le temps de calcul côté reconstruction géométrique, je vais évaluer :

1.  **[GLOMAP](https://github.com/colmap/glomap)** : Une alternative moderne et plus efficace à COLMAP.
2.  **[CF-3DGS](https://github.com/NVlabs/CF-3DGS)** : Une approche utilisant la continuité temporelle des flux vidéo pour contourner COLMAP.

#### 1. GLOMAP

Dû à un bug référencé [**ici**](https://github.com/colmap/glomap/issues/221) je ne peux pas l'utiliser pour l'instant. Cependant, à noté que le code nécéssite l'utilisation de COLMAP pour le feature matching et le matching spatial.

#### 2. CF-3DGS
