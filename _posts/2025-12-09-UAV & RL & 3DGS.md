---
layout: post
title: Reconstruction 3D Autonome (Part 1)
date: 2025-12-09
categories: [Robotics, RL, 3DGS]
tags: [UAV, Computer Vision, Optimization]
---

Ce nouveau projet vise à optimiser la trajectoire de vol d’un drone via l'Apprentissage par Renforcement (RL) afin d'améliorer la qualité et la vitesse d'une reconstruction 3D basée sur le 3D Gaussian Splatting (3DGS).

L'idée centrale repose sur l'efficacité : au lieu de scanner naïvement une zone entière (pattern "tondeuse à gazon"), le drone utilise un signal d’incertitude fourni par le modèle pour cibler les zones mal reconstruites. C'est le principe du Next-Best-View (NBV) Planning. L’objectif est de maximiser le gain d'information tout en minimisant le temps de vol.

**Objectif :** Tester le 3DGS pour obtenir une vision claire des contraintes et de la faisabilité du projet.

<figure style="text-align: center;">
    <img src="{{ site.baseurl }}/images/prometheus.gif" alt="Drone scanning simulation" width="100%" style="border-radius: 8px;">
    <figcaption><i>Je fixe une caméra 360° sur un drone et le tour est joué, probablement</i></figcaption>
</figure>

---

## Architecture du Pipeline

Pour atteindre cette autonomie, le système repose sur une architecture en boucle fermée où le traitement lourd est déporté sur une station de calcul fixe. Voici l’architecture *Perception-Action* envisagée :

<figure style="text-align: center;">
    <img src="{{ site.baseurl }}/images/UAV.png" alt="Architecture UAV RL" width="90%" style="display: block; margin: 0 auto;">
</figure>

Le pipeline débute par l'acquisition des flux vidéo et inertiels, lesquels alimentent une reconstruction 3DGS incrémentale dont l'analyse d'incertitude permet à l'agent RL de déterminer le prochain point de vue optimal.

---

## Analyse des premières expérimentations

La faisabilité d'un pilotage autonome repose sur la fluidité de la chaîne de traitement. Cette phase d'expérimentation vise donc à mesurer les performances brutes de la reconstruction afin de déterminer si les temps de calcul sont compatibles avec les exigences d'un vol drone, préalable indispensable à toute tentative de RL.



Une première référence a été établie via une approche mobile (Scaniverse / SuperSplat) sur un sujet simple comme ce [Panda roux](/scans/panda-roux.html). Si cette méthode est rapide et efficace, elle reste une "boîte noire" dépendante d'optimisations propriétaires et de traitements cloud, ce qui la rend difficilement transposable à un système embarqué autonome sous Linux.

Pour me rapprocher des conditions d'un drone porteur d'une optique de qualité, j'ai réalisé une série de tests avec un Fujifilm XT-2 (23mm f/2). Le traitement, effectué sur une station fixe (RX 7800XT), a immédiatement mis en évidence la lourdeur du calcul Structure-from-Motion (SfM). Qu'il s'agisse d'un matching exhaustif sur photos (environ 10 minutes pour 30 clichés) ou d'un matching séquentiel sur flux vidéo (environ 5 minutes), les temps de calcul restent incompatibles avec une boucle de contrôle réactive. 

<figure style="text-align: center;">
    <img src="/images/zig.png" alt="Visualisation Zig" width="100%" style="border-radius: 8px;">
    <figcaption><i>Scan test : "Zigoto" ou "Le Zig"</i></figcaption>
</figure>

Ces essais avec le XT-2 ont également souligné des limites physiques critiques. La focale de 23mm (eq. 35mm) s'est avérée être en limite basse pour la photogrammétrie, captant un volume d'environnement inutile qui surcharge le calcul. Plus problématique encore : l'absence de stabilisation (IBIS) sur le boîtier et l'objectif génère un flou de bougé qui dégrade instantanément la qualité des gaussiennes.

> Exemple de scan en mode vidéo, donc non stabilisé : [voir le scan](/scans/zigoto.html).

Le passage à l'échelle sur un appartement complet a soulevé des questions techniques importantes, notamment sur la compatibilité entre le pipeline COLMAP et mon GPU AMD (RX 7800XT). S'il est possible de basculer les calculs sur le CPU, les temps de traitement deviennent prohibitifs. Les crashs systématiques rencontrés sous WSL2 pourraient résulter d'une mauvaise configuration ou des limites de l'accélération matérielle non-NVIDIA. Ce constat, qui nécessite encore des recherches pour isoler les causes exactes (logicielle ou matérielle), renforce l'idée qu'un pivot vers une approche moins dépendante du SfM classique est une piste sérieuse à explorer.

---

## Pistes d'optimisation : L'alternative au SfM classique
L'analyse des premiers tests indique que le verrou technologique ne situe pas dans l'entraînement des gaussiennes, mais dans l'estimation de pose (SfM). Comme le souligne une discussion sur [Reddit](https://www.reddit.com/r/GaussianSplatting/comments/1mdjscn/how_is_the_scaniverse_app_even_possible/) concernant l'efficacité d'applications comme Scaniverse, l'astuce réside dans l'utilisation des données capteurs : en exploitant l'ordre des images et les données gyroscopiques, il est possible de s'affranchir de la majeure partie des calculs de structure.

Le drone disposant par définition de capteurs précis (IMU, odométrie, GPS), la stratégie consiste à initialiser les poses via la télémétrie ou à utiliser un SLAM visuel embarqué pour remplacer le pipeline COLMAP. Cette approche permettrait de fournir des poses "suffisamment bonnes" pour que l'optimisation 3DGS prenne le relais, sans passer par un matching exhaustif coûteux. 

### Solutions techniques à l'étude

Plusieurs alternatives *SfM-Free* ou optimisées ont capté mon attention :

Le projet [GLOMAP](https://github.com/colmap/glomap) se présente comme une alternative moderne à COLMAP pour le mapping global, promettant une accélération significative. Toutefois, son intégration est pour l'instant ralentie par un [bug connu](https://github.com/colmap/glomap/issues/221) et la nécessité de maintenir une étape préalable de feature matching.

En parallèle, j'évalue [CF-3DGS](https://github.com/NVlabs/CF-3DGS), une solution de pointe pour la reconstruction sans SfM. S'agissant d'un code de recherche, la question de sa stabilité et de sa portabilité pour une utilisation concrète reste entière.